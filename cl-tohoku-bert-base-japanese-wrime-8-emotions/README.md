# models

## ベースのモデル

### About Dataset

CC-100: 74.3GB
Wikipedia: 4.9GB

tokenize tool:fugashi,mecab-ipadic-NEologd

参考;
[BERT base Japanese (unidic-lite with whole word masking, CC-100 and jawiki-20230102)](https://huggingface.co/tohoku-nlp/bert-base-japanese-v3)

## 追加学習用のデータセット

> Ver.1: 80人の筆者から収集した43,200件の投稿に感情強度をラベル付けしました。
https://github.com/ids-cv/wrime

## 出来上がりモデル

Model repository
[model](https://huggingface.co/kynea0b/cl-tohoku-bert-base-japanese-v3-wrime-8-emotions)

## resultsには代表的な例

代表例となる結果たち。

## 追加実験

今後試したいデータセットたち or 追加学習したい
https://zenn.dev/panyoriokome/scraps/bb96bd0e512124

